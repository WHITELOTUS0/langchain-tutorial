{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bBNhXrQpXQpr",
        "outputId": "1e561139-2182-44c3-8ae3-3f7cf4fd1086"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.0.208\n",
            "  Downloading langchain-0.0.208-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting deeplake==3.9.27\n",
            "  Downloading deeplake-3.9.27.tar.gz (618 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m618.7/618.7 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting openai==0.27.8\n",
            "  Downloading openai-0.27.8-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (3.11.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (4.0.3)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.208)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting langchainplus-sdk>=0.0.13 (from langchain==0.0.208)\n",
            "  Downloading langchainplus_sdk-0.0.20-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (2.10.2)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (1.26.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain==0.0.208)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting pydantic<2,>=1 (from langchain==0.0.208)\n",
            "  Downloading pydantic-1.10.19-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (152 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.6/152.6 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (2.32.3)\n",
            "Collecting tenacity<9.0.0,>=8.1.0 (from langchain==0.0.208)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting pillow~=10.4.0 (from deeplake==3.9.27)\n",
            "  Downloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting boto3 (from deeplake==3.9.27)\n",
            "  Downloading boto3-1.35.90-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from deeplake==3.9.27) (8.1.7)\n",
            "Collecting pathos (from deeplake==3.9.27)\n",
            "  Downloading pathos-0.3.3-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting humbug>=0.3.1 (from deeplake==3.9.27)\n",
            "  Downloading humbug-0.3.2-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deeplake==3.9.27) (4.67.1)\n",
            "Collecting lz4 (from deeplake==3.9.27)\n",
            "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: pyjwt in /usr/local/lib/python3.10/dist-packages (from deeplake==3.9.27) (2.10.1)\n",
            "Collecting aioboto3>=10.4.0 (from deeplake==3.9.27)\n",
            "  Downloading aioboto3-13.3.0-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from deeplake==3.9.27) (1.6.0)\n",
            "Collecting libdeeplake==0.0.147 (from deeplake==3.9.27)\n",
            "  Downloading libdeeplake-0.0.147-cp310-cp310-manylinux2014_x86_64.whl.metadata (352 bytes)\n",
            "Collecting dill (from libdeeplake==0.0.147->deeplake==3.9.27)\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.11.6)\n",
            "Collecting aiobotocore==2.16.0 (from aiobotocore[boto3]==2.16.0->aioboto3>=10.4.0->deeplake==3.9.27)\n",
            "  Downloading aiobotocore-2.16.0-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting aiofiles>=23.2.1 (from aioboto3>=10.4.0->deeplake==3.9.27)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting botocore<1.35.82,>=1.35.74 (from aiobotocore==2.16.0->aiobotocore[boto3]==2.16.0->aioboto3>=10.4.0->deeplake==3.9.27)\n",
            "  Downloading botocore-1.35.81-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore==2.16.0->aiobotocore[boto3]==2.16.0->aioboto3>=10.4.0->deeplake==3.9.27) (1.17.0)\n",
            "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore==2.16.0->aiobotocore[boto3]==2.16.0->aioboto3>=10.4.0->deeplake==3.9.27)\n",
            "  Downloading aioitertools-0.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting boto3 (from deeplake==3.9.27)\n",
            "  Downloading boto3-1.35.81-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (1.18.3)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->deeplake==3.9.27)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->deeplake==3.9.27)\n",
            "  Downloading s3transfer-0.10.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.208)\n",
            "  Downloading marshmallow-3.23.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.208)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain==0.0.208) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.208) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.208) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.208) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.208) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.208) (3.1.1)\n",
            "Collecting ppft>=1.7.6.9 (from pathos->deeplake==3.9.27)\n",
            "  Downloading ppft-1.7.6.9-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pox>=0.3.5 (from pathos->deeplake==3.9.27)\n",
            "  Downloading pox-0.3.5-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting multiprocess>=0.70.17 (from pathos->deeplake==3.9.27)\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.82,>=1.35.74->aiobotocore==2.16.0->aiobotocore[boto3]==2.16.0->aioboto3>=10.4.0->deeplake==3.9.27) (2.8.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.208) (24.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.208)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.82,>=1.35.74->aiobotocore==2.16.0->aiobotocore[boto3]==2.16.0->aioboto3>=10.4.0->deeplake==3.9.27) (1.17.0)\n",
            "Downloading langchain-0.0.208-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libdeeplake-0.0.147-cp310-cp310-manylinux2014_x86_64.whl (17.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.7/17.7 MB\u001b[0m \u001b[31m117.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aioboto3-13.3.0-py3-none-any.whl (34 kB)\n",
            "Downloading aiobotocore-2.16.0-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.35.81-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Downloading humbug-0.3.2-py3-none-any.whl (15 kB)\n",
            "Downloading langchainplus_sdk-0.0.20-py3-none-any.whl (25 kB)\n",
            "Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-1.10.19-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathos-0.3.3-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading botocore-1.35.81-py3-none-any.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m127.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading marshmallow-3.23.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.17-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pox-0.3.5-py3-none-any.whl (29 kB)\n",
            "Downloading ppft-1.7.6.9-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3transfer-0.10.4-py3-none-any.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading aioitertools-0.12.0-py3-none-any.whl (24 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: deeplake\n",
            "  Building wheel for deeplake (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deeplake: filename=deeplake-3.9.27-py3-none-any.whl size=740550 sha256=5f9812fce7bb55b75f2731bd6ad516e7a56789183f80a16aa1407903c4fe3001\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/e6/93/ec5cbe3896a600183b67ca09014e694dacc494c344727cfdfe\n",
            "Successfully built deeplake\n",
            "Installing collected packages: tenacity, pydantic, ppft, pox, pillow, mypy-extensions, marshmallow, lz4, jmespath, dill, aioitertools, aiofiles, typing-inspect, tiktoken, openapi-schema-pydantic, multiprocess, libdeeplake, langchainplus-sdk, humbug, botocore, s3transfer, pathos, dataclasses-json, openai, langchain, boto3, aiobotocore, aioboto3, deeplake\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.10.3\n",
            "    Uninstalling pydantic-2.10.3:\n",
            "      Successfully uninstalled pydantic-2.10.3\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.0.0\n",
            "    Uninstalling pillow-11.0.0:\n",
            "      Successfully uninstalled pillow-11.0.0\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.57.4\n",
            "    Uninstalling openai-1.57.4:\n",
            "      Successfully uninstalled openai-1.57.4\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.12\n",
            "    Uninstalling langchain-0.3.12:\n",
            "      Successfully uninstalled langchain-0.3.12\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 1.4.20 requires pydantic>=2.7.0, but you have pydantic 1.10.19 which is incompatible.\n",
            "google-genai 0.3.0 requires pydantic<3.0.0dev,>=2.0.0, but you have pydantic 1.10.19 which is incompatible.\n",
            "langchain-core 0.3.25 requires pydantic<3.0.0,>=2.5.2; python_full_version < \"3.12.4\", but you have pydantic 1.10.19 which is incompatible.\n",
            "wandb 0.19.1 requires pydantic<3,>=2.6, but you have pydantic 1.10.19 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aioboto3-13.3.0 aiobotocore-2.16.0 aiofiles-24.1.0 aioitertools-0.12.0 boto3-1.35.81 botocore-1.35.81 dataclasses-json-0.5.14 deeplake-3.9.27 dill-0.3.9 humbug-0.3.2 jmespath-1.0.1 langchain-0.0.208 langchainplus-sdk-0.0.20 libdeeplake-0.0.147 lz4-4.3.3 marshmallow-3.23.2 multiprocess-0.70.17 mypy-extensions-1.0.0 openai-0.27.8 openapi-schema-pydantic-1.2.4 pathos-0.3.3 pillow-10.4.0 pox-0.3.5 ppft-1.7.6.9 pydantic-1.10.19 s3transfer-0.10.4 tenacity-8.5.0 tiktoken-0.8.0 typing-inspect-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "19315646db7d4cb9a20fc791eceadcf2"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install langchain==0.0.208 deeplake==3.9.27 openai==0.27.8 tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.llms import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get the API key from Colab's userdata\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Set it as an environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "\n",
        "# Now initialize LangChain\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0.9)"
      ],
      "metadata": {
        "id": "1jbphU5mYog6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Suggest a personalized workout routine for someone looking to improve cardiovascular endurance and prefers outdoor activities.\"\n",
        "print(llm(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0ZUvqo5cBGz",
        "outputId": "c3efc651-7ac0-48f3-b6f4-a3ed728beac2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Monday:\n",
            "Warm up: 5-10 minutes of brisk walking or light jog\n",
            "\n",
            "Circuit training:\n",
            "- 10 push-ups\n",
            "- 10 squats\n",
            "- 10 lunges (each leg)\n",
            "- 30 seconds of high knees\n",
            "- 10 mountain climbers\n",
            "Repeat circuit 3 times\n",
            "\n",
            "Outdoor activity:\n",
            "- 30 minute hike or trail run\n",
            "\n",
            "Tuesday:\n",
            "Warm up: 5-10 minutes of dynamic stretches (leg swings, arm circles, etc.)\n",
            "\n",
            "Cardio:\n",
            "- 25 minutes of cycling\n",
            "- 10 minutes of jump rope\n",
            "- 15 minutes of stair climbing\n",
            "- 5 minutes of sprints (30 seconds on, 30 seconds off)\n",
            "Cool down: 5-10 minutes of walking\n",
            "\n",
            "Wednesday:\n",
            "Active rest day:\n",
            "- Take a leisurely bike ride or walk in nature for 30-45 minutes\n",
            "\n",
            "Thursday:\n",
            "Warm up: 5-10 minutes of light jog and dynamic stretches\n",
            "\n",
            "HIIT (High Intensity Interval Training):\n",
            "- 1 minute of burpees\n",
            "- 1 minute of jumping jacks\n",
            "- 1 minute of high knees\n",
            "- 1 minute of mountain climbers\n",
            "- 1 minute of rest\n",
            "Repeat circuit 3 times\n",
            "\n",
            "Outdoor activity:\n",
            "- 30 minutes of swimming\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chains\n",
        "\n",
        "In LangChain, a chain is an end-to-end wrapper around multiple individual components, providing a way to accomplish a common use case by combining these components in a specific sequence. The most commonly used type of chain is the `LLMChain`, which consists of a `PromptTemplate`, a model (either an LLM or a ChatModel), and an optional output parser.\n",
        "\n",
        "The `LLMChain` works as follows:\n",
        "\n",
        "1. Takes (multiple) input variables.\n",
        "2. Uses the `PromptTemplate` to format the input variables into a prompt.\n",
        "3. Passes the formatted prompt to the model (LLM or ChatModel).\n",
        "4. If an output parser is provided, it uses the `OutputParser` to parse the output of the LLM into a final format.\n",
        "\n",
        "In the next example, we demonstrate how to create a chain that generates a possible name for a company that produces eco-friendly water bottles. By using LangChain's LLMChain, `PromptTemplate`, and `OpenAI` classes, we can easily define our prompt, set the input variables, and generate creative outputs."
      ],
      "metadata": {
        "id": "IMiGVpppjOMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0.9)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"What is a good name for a company that makes {product}?\",\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "print(chain.run(\"eco-friendly water bottles\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSI2cfsHj96u",
        "outputId": "80093bde-d8f1-4907-d4e5-d2dbb15ff6b9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. GreenWave Bottles\n",
            "2. EarthEco Water\n",
            "3. PureCycle Bottles\n",
            "4. Sustainable Sips\n",
            "5. EcoHydrate Co.\n",
            "6. ReNewBottle Co.\n",
            "7. PlanetPak Bottles\n",
            "8. EcoAqua Solutions\n",
            "9. EarthSwell Bottles\n",
            "10. CleanCanteen Co.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Memory\n",
        "\n",
        "In LangChain, Memory refers to the mechanism that stores and manages the conversation history between a user and the AI. It helps maintain context and coherency throughout the interaction, enabling the AI to generate more relevant and accurate responses. Memory, such as `ConversationBufferMemory`, acts as a wrapper around `ChatMessageHistory`, extracting the messages and providing them to the chain for better context-aware generation."
      ],
      "metadata": {
        "id": "3EP36e3crJ7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    memory=ConversationBufferMemory()\n",
        ")\n",
        "\n",
        "# Start the conversation\n",
        "conversation.predict(input=\"Tell me about yourself.\")\n",
        "\n",
        "# Continue the conversation\n",
        "conversation.predict(input=\"What can you do?\")\n",
        "conversation.predict(input=\"How can you help me with data analysis?\")\n",
        "\n",
        "# Display the conversation\n",
        "print(conversation.memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATMwRKK2rQ6m",
        "outputId": "96ad0037-9e11-49c5-b8b0-edd5a532d23f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Tell me about yourself.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Tell me about yourself.\n",
            "AI:  Well, I am an artificial intelligence program designed and created by a team of programmers. I am constantly learning and improving my abilities through algorithms and data analysis. My main purpose is to assist and provide information to users like yourself. I am currently housed in a server located in a data center, but I can also be accessed through various devices such as computers, smartphones, and smart speakers. Is there anything specific you would like to know about me?\n",
            "Human: What can you do?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Tell me about yourself.\n",
            "AI:  Well, I am an artificial intelligence program designed and created by a team of programmers. I am constantly learning and improving my abilities through algorithms and data analysis. My main purpose is to assist and provide information to users like yourself. I am currently housed in a server located in a data center, but I can also be accessed through various devices such as computers, smartphones, and smart speakers. Is there anything specific you would like to know about me?\n",
            "Human: What can you do?\n",
            "AI:  I have a wide range of capabilities, but some of my main functions include answering questions, performing tasks, and providing recommendations based on data analysis. I can also understand and respond to natural language, making it easier for users to communicate with me. Additionally, I am constantly learning and updating my knowledge base, so my abilities are constantly expanding. Is there something specific you would like me to do for you?\n",
            "Human: How can you help me with data analysis?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Human: Tell me about yourself.\n",
            "AI:  Well, I am an artificial intelligence program designed and created by a team of programmers. I am constantly learning and improving my abilities through algorithms and data analysis. My main purpose is to assist and provide information to users like yourself. I am currently housed in a server located in a data center, but I can also be accessed through various devices such as computers, smartphones, and smart speakers. Is there anything specific you would like to know about me?\n",
            "Human: What can you do?\n",
            "AI:  I have a wide range of capabilities, but some of my main functions include answering questions, performing tasks, and providing recommendations based on data analysis. I can also understand and respond to natural language, making it easier for users to communicate with me. Additionally, I am constantly learning and updating my knowledge base, so my abilities are constantly expanding. Is there something specific you would like me to do for you?\n",
            "Human: How can you help me with data analysis?\n",
            "AI:  I can assist with data analysis by quickly sorting through large amounts of data and identifying patterns and trends. I can also provide visualizations and reports to help you better understand the data. Additionally, I can make predictions and recommendations based on the data, which can be useful for decision making. Is there a specific dataset or analysis you would like me to help with?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deeplake Vector store"
      ],
      "metadata": {
        "id": "Xwp5Rrgh2K9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the activeloop token from Colab's userdata\n",
        "activeloop_token = userdata.get('ACTIVELOOP_TOKEN')\n",
        "\n",
        "# Set it as an environment variable\n",
        "os.environ[\"ACTIVELOOP_TOKEN\"] = activeloop_token\n",
        "\n",
        "activeloop_org_id = userdata.get('ACTIVELOOP_ORG_ID')\n",
        "\n",
        "os.environ[\"ACTIVELOOP_ORG_ID\"] = activeloop_org_id"
      ],
      "metadata": {
        "id": "RdDEyt7P1pCf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import DeepLake\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "texts = [\n",
        "    \"Glorry Sibomana was born on 21 January 2003\",\n",
        "    \"Louis XIV was born in 5 September 1638\"\n",
        "]\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.create_documents(texts)\n",
        "\n",
        "my_activeloop_org_id =  activeloop_org_id\n",
        "my_activeloop_dataset_name = \"langchain_course_from_zero_to_hero\"\n",
        "\n",
        "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
        "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
        "\n",
        "# add documents to our Deep Lake dataset\n",
        "db.add_documents(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfrXBGGn4_hc",
        "outputId": "ea8cc5f5-2e30-40a4-9db3-240205e8a657"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Deep Lake dataset has been successfully created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating 2 embeddings in 1 batches of size 2:: 100%|██████████| 1/1 [00:09<00:00,  9.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset(path='hub://glorrysibomana758/langchain_course_from_zero_to_hero', tensors=['text', 'metadata', 'embedding', 'id'])\n",
            "\n",
            "  tensor      htype      shape     dtype  compression\n",
            "  -------    -------    -------   -------  ------- \n",
            "   text       text      (2, 1)      str     None   \n",
            " metadata     json      (2, 1)      str     None   \n",
            " embedding  embedding  (2, 1536)  float32   None   \n",
            "    id        text      (2, 1)      str     None   \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['8ed9e910-c6b8-11ef-b8f8-0242ac1c000c',\n",
              " '8ed9ea50-c6b8-11ef-b8f8-0242ac1c000c']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's create a `RetrievalQA` chain:"
      ],
      "metadata": {
        "id": "sLtoKmm67kd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_qa = RetrievalQA.from_chain_type(\n",
        "\tllm=llm,\n",
        "\tchain_type=\"stuff\",\n",
        "\tretriever=db.as_retriever()\n",
        ")"
      ],
      "metadata": {
        "id": "Tll72vbg7n0L"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " let's create an agent that uses the `RetrievalQA` chain as a tool:"
      ],
      "metadata": {
        "id": "9Ad4KuI88JQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.agents import AgentType\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Retrieval QA System\",\n",
        "        func=retrieval_qa.run,\n",
        "        description=\"Useful for answering questions.\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "agent = initialize_agent(\n",
        "\ttools,\n",
        "\tllm,\n",
        "\tagent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "\tverbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "5LdLRDy78L8l"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can use the agent to ask a question:"
      ],
      "metadata": {
        "id": "GIE4D6C_8et9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.run(\"When was Glorry born?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oT78Yosh8gAU",
        "outputId": "512a492e-a11b-4629-b916-c57ea71911a9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should use the Retrieval QA System to find the answer\n",
            "Action: Retrieval QA System\n",
            "Action Input: \"When was Glorry born?\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m Glorry was born on 21 January 2003.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: Glorry was born on 21 January 2003.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Glorry was born on 21 January 2003.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s add an example of reloading an existing vector store and adding more data.\n",
        "\n",
        "We first reload an existing vector store from Deep Lake that's located at a specified dataset path. Then, we load new textual data and split it into manageable chunks. Finally, we add these chunks to the existing dataset, creating and storing corresponding embeddings for each added text segment:\n",
        "\n"
      ],
      "metadata": {
        "id": "4M0_hI_e-U8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the existing Deep Lake dataset and specify the embedding function\n",
        "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
        "\n",
        "# create new documents\n",
        "texts = [\n",
        "    \"Kato steven was born on 19th Feb 1997\",\n",
        "    \"Kimberly Ann Nambugu was born on 21st March 2000\"\n",
        "]\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.create_documents(texts)\n",
        "\n",
        "# add documents to our Deep Lake dataset\n",
        "db.add_documents(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60Cvm49v-Wpm",
        "outputId": "df9d13cb-d83e-4311-f1b2-bb47c1cc749d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep Lake Dataset in hub://glorrysibomana758/langchain_course_from_zero_to_hero already exists, loading from the storage\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating 2 embeddings in 1 batches of size 2:: 100%|██████████| 1/1 [00:09<00:00,  9.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset(path='hub://glorrysibomana758/langchain_course_from_zero_to_hero', tensors=['embedding', 'id', 'metadata', 'text'])\n",
            "\n",
            "  tensor      htype      shape     dtype  compression\n",
            "  -------    -------    -------   -------  ------- \n",
            " embedding  embedding  (4, 1536)  float32   None   \n",
            "    id        text      (4, 1)      str     None   \n",
            " metadata     json      (4, 1)      str     None   \n",
            "   text       text      (4, 1)      str     None   \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cc447f2a-c6ba-11ef-b8f8-0242ac1c000c',\n",
              " 'cc448092-c6ba-11ef-b8f8-0242ac1c000c']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then recreate our previous agent and ask a question that can be answered only by the last documents added."
      ],
      "metadata": {
        "id": "iEoq6Jxe_JCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate the wrapper class for GPT3\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
        "\n",
        "# create a retriever from the db\n",
        "retrieval_qa = RetrievalQA.from_chain_type(\n",
        "\tllm=llm, chain_type=\"stuff\", retriever=db.as_retriever()\n",
        ")\n",
        "\n",
        "# instantiate a tool that uses the retriever\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Retrieval QA System\",\n",
        "        func=retrieval_qa.run,\n",
        "        description=\"Useful for answering questions.\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "# create an agent that uses the tool\n",
        "agent = initialize_agent(\n",
        "\ttools,\n",
        "\tllm,\n",
        "\tagent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "\tverbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "9OcaLO1A_LmX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s now test our agent with a new question."
      ],
      "metadata": {
        "id": "KVXp79m1_VG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.run(\"When was Kato steven born?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWevgX_r_ZIY",
        "outputId": "9be87947-2051-407a-fcf9-b1e5270cfe10"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should use the Retrieval QA System to find the answer\n",
            "Action: Retrieval QA System\n",
            "Action Input: \"When was Kato Steven born?\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m Kato Steven was born on 19th Feb 1997.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: Kato Steven was born on 19th Feb 1997.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Kato Steven was born on 19th Feb 1997.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agents in LangChain\n",
        "In LangChain, agents are high-level components that use language models (LLMs) to determine which actions to take and in what order. An action can either be using a tool and observing its output or returning it to the user. Tools are functions that perform specific duties, such as Google Search, database lookups, or Python REPL.\n",
        "\n",
        "Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done.\n",
        "\n",
        "Several types of agents are available in LangChain:\n",
        "\n",
        "1. The `zero-shot-react-description` agent uses the ReAct framework to decide which tool to employ based purely on the tool's description. It necessitates a description of each tool.\n",
        "2. The `react-docstore` agent engages with a docstore through the ReAct framework. It needs two tools: a Search tool and a Lookup tool. The Search tool finds a document, and the Lookup tool searches for a term in the most recently discovered document.\n",
        "3. The `self-ask-with-search` agent employs a single tool named Intermediate Answer, which is capable of looking up factual responses to queries. It is identical to the original self-ask with the search paper, where a Google search API was provided as the tool.\n",
        "4. The `conversational-react-description` agent is designed for conversational situations. It uses the ReAct framework to select a tool and uses memory to remember past conversation interactions.\n",
        "\n",
        "In our example, the Agent will use the Google Search tool to look up recent information about the Mars rover and generates a response based on this information.\n",
        "\n",
        "First, you want to set the environment variables “GOOGLE_API_KEY” and “GOOGLE_CSE_ID” to be able to use Google Search via API."
      ],
      "metadata": {
        "id": "TFzvynwdGzfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "from langchain.agents import AgentType\n",
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "\n",
        "from langchain.agents import Tool\n",
        "from langchain.utilities import GoogleSearchAPIWrapper\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
        "\n",
        "# remember to set the environment variables\n",
        "# “GOOGLE_API_KEY” and “GOOGLE_CSE_ID” to be able to use\n",
        "# Google Search via API.\n",
        "# Get the key from Colab's userdata\n",
        "google_api_key = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Set it as an environment variable\n",
        "os.environ[\"GOOGLE_API_KEY\"] = google_api_key\n",
        "\n",
        "google_cse_id = userdata.get('GOOGLE_CSE_ID')\n",
        "\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = google_cse_id\n",
        "\n",
        "search = GoogleSearchAPIWrapper()"
      ],
      "metadata": {
        "id": "qWKsLrUEHEgH"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Tool` object represents a specific capability or function the system can use. In this case, it's a tool for performing Google searches.\n",
        "\n",
        "It is initialized with three parameters:\n",
        "\n",
        "- `name` parameter: This is a string that serves as a unique identifier for the tool. In this case, the name of the tool is \"google-search.”\n",
        "- `func` parameter: This parameter is assigned the function that the tool will execute when called. In this case, it's the `run` method of the `search` object, which presumably performs a Google search.\n",
        "- `description` parameter: This is a string that briefly explains what the tool does. The description explains that this tool is helpful when you need to use Google to answer questions about current events."
      ],
      "metadata": {
        "id": "8LxtRPfFJ8Z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    Tool(\n",
        "        name = \"google-search\",\n",
        "        func=search.run,\n",
        "        description=\"useful for when you need to search google to answer questions about current events\"\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "1_-5SIGgKNTv"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we create an agent that uses our Google Search tool:\n",
        "\n",
        "- `initialize_agent()`: This function call creates and initializes an agent. An agent is a component that determines which actions to take based on user input. These actions can be using a tool, returning a response to the user, or something else.\n",
        "- `tools:`  represents the list of Tool objects that the agent can use.\n",
        "- `agent=\"zero-shot-react-description\"`: The \"zero-shot-react-description\" type of an Agent uses the ReAct framework to decide which tool to use based only on the tool's description.\n",
        "- `verbose=True`: when set to True, it will cause the Agent to print more detailed information about what it's doing. This is useful for debugging and understanding what's happening under the hood.\n",
        "- `max_iterations=6`: sets a limit on the number of iterations the Agent can perform before stopping. It's a way of preventing the agent from running indefinitely in some cases, which may have unwanted monetary costs."
      ],
      "metadata": {
        "id": "bCpW5l5ULsgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = initialize_agent(tools,\n",
        "                         llm,\n",
        "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "                         verbose=True,\n",
        "                         max_iterations=6)"
      ],
      "metadata": {
        "id": "DMZSGEjdL-zD"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now, we can check out the response:"
      ],
      "metadata": {
        "id": "wiWssW6BMIyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent(\"What's the latest news about the plane crash in South Korea?\")\n",
        "print(response['output'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6ne0KXTMJo8",
        "outputId": "4b924940-575a-4831-c2fb-f1629b301b05"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should search for news articles about the plane crash\n",
            "Action: google-search\n",
            "Action Input: \"South Korea plane crash\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m22 hours ago ... A video captured the moments before a plane carrying 181 passengers crashed while landing in an airport in South Korea. 2 days ago ... More than 170 killed after South Korean jet crash-lands at airport. Here's what we know · Efforts are made to lift the wreckage of an aircraft ... 10 hours ago ... A total of 179 of the 181 people travelling on the Boeing 737-800 were killed, with just two survivors - both cabin staff - pulled from the ... 5 hours ago ... Boeing Shares Drop After South Korean Crash ... A widely used Boeing aircraft, the 737-800, was involved in Sunday's crash-landing of a Jeju Air ... 12 hours ago ... The Jeju Air crash in South Korea on Sunday is the deadliest aviation disaster to hit the country since 1997, when a Korean Airlines flight ... 12 hours ago ... The crash killed 179 people, the South Korean fire agency said. Emergency workers pulled two crew members to safety. They were conscious and did ... Korean Air's deadliest incident was Flight 007 which was shot down by the Soviet Union on September 1, 1983. All 269 people on board were killed, including a ... 2 days ago ... The crash killed 179 people, the South Korean fire agency said. Emergency workers pulled two crew members, to safety. They were conscious and ... 12 hours ago ... Jeju Air flight 7C 2216 from Bangkok was carrying 175 passengers and six crew when disaster struck at the airport in Muan county, on the ... 8 hours ago ... A plane crash has killed almost everyone on board the South Korean Jeju Air Flight 2216. Video Duration 01 minutes 26 seconds 01:26.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m This is a tragic event and it seems like there are still updates coming in.\n",
            "Action: google-search\n",
            "Action Input: \"latest updates on South Korea plane crash\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m12 hours ago ... The Jeju Air crash in South Korea on Sunday is the deadliest aviation disaster to hit the country since 1997, when a Korean Airlines flight ... 1 day ago ... A plane carrying 181 passengers has crashed at an airport in South Korea, killing at least 62 people, the country's fire service has said; The ... 7 hours ago ... Officials have confirmed that 179 of the 181 passengers and crew died when the Jeju Air plane crashed into a wall at Muan international airport ... 2 days ago ... A passenger plane crashed while landing at an airport in South Korea on Sunday, killing 179 of the 181 people on board in the worst aviation ... 24 hours ago ... 179 people were killed as a plane crashed while landing at South Korea's Muan International Airport. · The Boeing 737-800 Jeju Air flight, ... 1 day ago ... A passenger plane burst into flames Sunday after it skidded off a runway at a South Korean airport and slammed into a concrete fence when its ... 2 days ago ... A plane carrying 181 passengers has crashed at an airport in South Korea, killing at least 62 people, the country's fire service has said; The ... 22 hours ago ... Experts question 'bird strike' as reason behind South Korea plane crash that killed 179 | Top updates. ByHT News Desk. Dec 29, 2024 10:57 PM ... 1 day ago ... South Korea Plane Crash: Death Count Confirmed To 151. South Korea's fire agency said that the death count from a Jeju Air plane crash has been ... 2 days ago ... We'll leave the live blog here, but we'll bring you any major updates on the ABC News website. You can read more about the latest on the crash ...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m It seems like the death toll has been confirmed to be 179.\n",
            "Action: google-search\n",
            "Action Input: \"death toll confirmed for South Korea plane crash\"\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Observation: \u001b[36;1m\u001b[1;3m18 hours ago ... The passenger plane crash that killed 179 in South Korea comes at a time of ... confirmed two Thai passengers were among the fatalities. In ... 24 hours ago ... Reporting from Seoul, South Korea. The National Fire Agency has confirmed a death toll of 167 so far. Officials are raising the death toll as ... 1 day ago ... Death toll from plane fire at South Korean airport rises to 120South Korea's national fire agency says 120 people have been confirmed dead after ... 9 hours ago ... SEOUL, South Korea — Two people survived and 179 were confirmed ... As the confirmed death toll ticked up, anxious families gathered at ... 12 hours ago ... It is the deadliest aviation disaster to hit South Korea since 1997, when a Korean Airlines Boeing 747 crashed in the Guam jungle, leaving 228 ... 23 hours ago ... South Korea plane crash live: 151 of 181 pax confirmed dead, toll likely to rise. Confirmed death toll of the 'Jeju Air Flight 2216 Accident ... 2 days ago ... Reporting from Muan, South Korea. The official death toll is 179, fire officials confirmed at the Muan airport. All passengers and crew have ... 1 day ago ... Firefighters now say all people on board except two rescued are presumed dead. Follow Live. Key Events. Death toll from crash rises ... 1 day ago ... ... killed in a Jeju Air plane crash at South Korea's Muan International Airport on Sunday ... Those confirmed dead include 37 women and 25 men; The ... 2 days ago ... By Jesmine Cheong. The South Korean fire department has updated the death toll to 167.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m It's heartbreaking to see so many lives lost in this tragedy.\n",
            "Final Answer: The latest news about the plane crash in South Korea is that the death toll has been confirmed to be 179 people.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "The latest news about the plane crash in South Korea is that the death toll has been confirmed to be 179 people.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summary, Agents in LangChain help decide which actions to take based on user input. The example demonstrates initializing and using a \"zero-shot-react-description\" agent with a Google search tool."
      ],
      "metadata": {
        "id": "D9KDfEoWQcH4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools in LangChain\n",
        "\n",
        "LangChain provides a variety of tools for agents to interact with the outside world. These tools can be used to create custom agents that perform various tasks, such as searching the web, answering questions, or running Python code. In this section, we will discuss the different tool types available in LangChain and provide examples of creating and using them.\n",
        "\n",
        "In our example, two tools are being defined for use within a LangChain agent: a Google Search tool and a Language Model tool acting specifically as a text summarizer. The Google Search tool, using the GoogleSearchAPIWrapper, will handle queries that involve finding recent event information. The Language Model tool leverages the capabilities of a language model to summarize texts. These tools are designed to be used interchangeably by the agent, depending on the nature of the user's query.\n",
        "\n",
        "Let’s import the necessary libraries."
      ],
      "metadata": {
        "id": "cHIyaUBBQdrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.agents import Tool\n",
        "from langchain.utilities import GoogleSearchAPIWrapper\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.agents import initialize_agent, AgentType"
      ],
      "metadata": {
        "id": "RAcDCA6lQuTq"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then instantiate a  `LLMChain` specifically for text summarization."
      ],
      "metadata": {
        "id": "3dwfhtD4STNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=\"Write a summary of the following text: {query}\"\n",
        ")\n",
        "\n",
        "summarize_chain = LLMChain(llm=llm, prompt=prompt)"
      ],
      "metadata": {
        "id": "tZnoosGLSZ5L"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we create the tools that our agent will use."
      ],
      "metadata": {
        "id": "IuB6kDtkSxEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remember to set the environment variables\n",
        "# “GOOGLE_API_KEY” and “GOOGLE_CSE_ID” to be able to use\n",
        "# Google Search via API. Check above\n",
        "search = GoogleSearchAPIWrapper()\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Search\",\n",
        "        func=search.run,\n",
        "        description=\"useful for finding information about recent events\"\n",
        "    ),\n",
        "    Tool(\n",
        "       name='Summarizer',\n",
        "       func=summarize_chain.run,\n",
        "       description='useful for summarizing texts'\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "izBHffL3SyKh"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now ready to create our agent that leverages two tools."
      ],
      "metadata": {
        "id": "6bGuB3FZTKXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "uIpX4bF5TLmP"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s run the agent with a question about summarizing the latest news about the Mars rover."
      ],
      "metadata": {
        "id": "fqywPuWUTP2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent(\"What's the latest news about Jimmy carter death? Then please summarize the results.\")\n",
        "print(response['output'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8isWBN-Tm8l",
        "outputId": "bcd27ac9-d882-469a-fc12-c90fbf1ff866"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should use the search tool to find information about recent events.\n",
            "Action: Search\n",
            "Action Input: \"Jimmy Carter death\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m19 hours ago ... 29, 2024) — Jimmy Carter, 39th president of the United States and winner of the 2002 Nobel Peace Prize, died peacefully Sunday, Dec. 29, at ... 13 hours ago ... It is my solemn duty to announce officially the death of James Earl Carter, Jr., the thirty-ninth President of the United States, on December 29 ... James Earl Carter Jr. (October 1, 1924 – December 29, 2024) was an American politician and humanitarian who served as the 39th president of the United ... 19 hours ago ... Carter, the 39th president of the United States, died in Plains on Sunday at age 100. Carter was president from 1977 to 1981, but he was ... 8 hours ago ... Former US President Jimmy Carter has died aged 100, the centre he founded has confirmed. The former peanut farmer lived longer than any ... 18 hours ago ... President Carter died peacefully Sunday, Dec. 29, at his home in Plains, Georgia, surrounded by his family. “We are deeply saddened by President ... 12 hours ago ... Carter, who lived longer than any president in history, died on Sunday afternoon at his home in Plains, Georgia. Biden described his fellow ... 19 hours ago ... Carter \"died peacefully Sunday, Dec. 29, at his home in Plains, Georgia, surrounded by his family,\" the Carter Center announced in a statement. 17 hours ago ... Former President Jimmy Carter has died at the age of 100. The 39th president of the United States was a Georgia peanut farmer who sought to ... 12 hours ago ... “On this day, December 29, 2024, the date of his death, we honor the memory of Georgia native, Jimmy Carter, as a revered community activist, ...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I should use the summarizer tool to summarize the results.\n",
            "Action: Summarizer\n",
            "Action Input: \"19 hours ago ... 29, 2024) — Jimmy Carter, 39th president of the United States and winner of the 2002 Nobel Peace Prize, died peacefully Sunday, Dec. 29, at ... 13 hours ago ... It is my solemn duty to announce officially the death of James Earl Carter, Jr., the thirty-ninth President of the United States, on December 29 ... James Earl Carter Jr. (October 1, 1924 – December 29, 2024) was an American politician and humanitarian who served as the 39th president of the United ... 19 hours ago ... Carter, the 39th president of the United States, died in Plains on Sunday at age 100. Carter was president from 1977 to 1981, but he was ... 8 hours ago ... Former US President Jimmy Carter has died aged 100, the centre he founded has confirmed. The former peanut farmer lived longer than any ... 18 hours ago ... President Carter died peacefully Sunday, Dec. 29, at his home in Plains, Georgia, surrounded by his family. “We are\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3m heartbroken to share the news that former US President Jimmy Carter passed away at the age of 100 on December 29, 2024. Carter, who served as the 39th president of the United States and won the Nobel Peace Prize in 2002, died peacefully at his home in Plains, Georgia. His death was announced by the Carter Center, the humanitarian organization he founded. Carter's death was also officially announced by the White House, with President Joe Biden calling him a \"giant of American history\" and ordering flags to be flown at half-staff in his honor. Carter, who was 100 years old, served as president from 1977 to 1981 and was known for his humanitarian work and efforts towards peace. He passed away surrounded by his family in his home in Plains, Georgia.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
            "Final Answer: Former US President Jimmy Carter passed away at the age of 100 on December 29, 2024.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Former US President Jimmy Carter passed away at the age of 100 on December 29, 2024.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how the agents used at first the “Search” tool to look for recent information about the Mars rover and then used the “Summarizer” tool for writing a summary.\n",
        "\n",
        "LangChain provides an expansive toolkit that integrates various functions to improve the functionality of conversational agents. Here are some examples:\n",
        "\n",
        "- `SerpAPI`: This tool is an interface for the SerpAPI search engine, allowing the agent to perform robust online searches to pull in relevant data for a conversation or task.\n",
        "- `PythonREPLTool`: This unique tool enables the writing and execution of Python code within an agent. This opens up a wide range of possibilities for advanced computations and interactions within the conversation.\n",
        "\n",
        "If you wish to add more specialized capabilities to your LangChain conversational agent, the platform offers the flexibility to create custom tools. By following the general tool creation guidelines provided in the LangChain documentation, you can develop tools tailored to the specific needs of your application."
      ],
      "metadata": {
        "id": "PReK_cG4UqTN"
      }
    }
  ]
}